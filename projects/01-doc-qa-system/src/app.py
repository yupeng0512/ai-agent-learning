"""
æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ - ä¸»åº”ç”¨

åŠŸèƒ½ï¼š
- ä¸Šä¼ æ–‡æ¡£ï¼ˆPDF/Markdown/TXTï¼‰
- æ„å»ºçŸ¥è¯†åº“
- æ™ºèƒ½é—®ç­”
- æ˜¾ç¤ºå¼•ç”¨æ¥æº

è¿è¡Œï¼š
cd projects/01-doc-qa-system
pip install -r requirements.txt
python src/app.py
"""

import os
import sys
import tempfile
from pathlib import Path
from typing import List, Tuple, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆå°è¯•å¤šä¸ªè·¯å¾„ï¼‰
env_paths = [
    Path(__file__).parent.parent.parent / ".env",  # ai-agent-learning/.env
    Path(__file__).parent.parent / ".env",          # 01-doc-qa-system/.env
    Path.cwd() / ".env",                            # å½“å‰ç›®å½•
]
for env_path in env_paths:
    if env_path.exists():
        load_dotenv(dotenv_path=env_path)
        print(f"âœ… åŠ è½½ç¯å¢ƒå˜é‡: {env_path}")
        break

# API é…ç½®
IFLOW_API_KEY = os.getenv("IFLOW_API_KEY")
IFLOW_BASE_URL = os.getenv("IFLOW_BASE_URL", "https://apis.iflow.cn/v1")
IFLOW_MODEL = os.getenv("IFLOW_MODEL", "qwen3-coder-plus")

SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
SILICONFLOW_BASE_URL = os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1")
SILICONFLOW_EMBEDDING_MODEL = os.getenv("SILICONFLOW_EMBEDDING_MODEL", "BAAI/bge-m3")


class DocQAApp:
    """æ–‡æ¡£é—®ç­”åº”ç”¨"""
    
    def __init__(self):
        self.llm = None
        self.embeddings = None
        self.vector_store = None
        self.qa_engine = None
        self.documents = []
        
        self._init_models()
    
    def _init_models(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        from langchain_openai import ChatOpenAI, OpenAIEmbeddings
        
        # æ£€æŸ¥ API Key
        if not IFLOW_API_KEY:
            raise ValueError("è¯·é…ç½® IFLOW_API_KEY")
        if not SILICONFLOW_API_KEY:
            raise ValueError("è¯·é…ç½® SILICONFLOW_API_KEY")
        
        # åˆ›å»º LLMï¼ˆç”¨äºå¯¹è¯ï¼‰
        self.llm = ChatOpenAI(
            model=IFLOW_MODEL,
            openai_api_key=IFLOW_API_KEY,
            openai_api_base=IFLOW_BASE_URL,
            temperature=0,
        )
        
        # åˆ›å»º Embeddingï¼ˆç”¨äºå‘é‡åŒ–ï¼‰
        self.embeddings = OpenAIEmbeddings(
            model=SILICONFLOW_EMBEDDING_MODEL,
            openai_api_key=SILICONFLOW_API_KEY,
            openai_api_base=SILICONFLOW_BASE_URL,
        )
        
        print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   å¯¹è¯æ¨¡å‹: {IFLOW_MODEL}")
        print(f"   Embedding: {SILICONFLOW_EMBEDDING_MODEL}")
    
    def upload_files(self, files: List) -> str:
        """
        ä¸Šä¼ å¹¶å¤„ç†æ–‡ä»¶
        
        Args:
            files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
            
        Returns:
            å¤„ç†ç»“æœæ¶ˆæ¯
        """
        from document_loader import DocumentLoader
        from vector_store import VectorStore
        
        if not files:
            return "âŒ è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶"
        
        loader = DocumentLoader(chunk_size=500, chunk_overlap=50)
        all_docs = []
        results = []
        
        for file in files:
            try:
                # Gradio è¿”å›çš„æ˜¯ä¸´æ—¶æ–‡ä»¶è·¯å¾„
                file_path = file.name if hasattr(file, 'name') else file
                docs = loader.load_file(file_path)
                all_docs.extend(docs)
                results.append(f"âœ… {Path(file_path).name}: {len(docs)} å—")
            except Exception as e:
                results.append(f"âŒ {Path(file_path).name}: {str(e)}")
        
        if all_docs:
            # åˆ›å»ºæˆ–æ›´æ–°å‘é‡æ•°æ®åº“
            if self.vector_store is None:
                self.vector_store = VectorStore(self.embeddings)
            
            self.vector_store.add_documents(all_docs)
            self.documents.extend(all_docs)
            
            # åˆ›å»ºé—®ç­”å¼•æ“
            from qa_engine import QAEngine
            self.qa_engine = QAEngine(
                self.llm,
                self.vector_store.as_retriever(search_kwargs={"k": 3}),
            )
            
            results.append(f"\nğŸ“Š çŸ¥è¯†åº“çŠ¶æ€: å…± {len(self.documents)} ä¸ªæ–‡æ¡£å—")
        
        return "\n".join(results)
    
    def upload_text(self, text: str, source_name: str = "ç”¨æˆ·è¾“å…¥") -> str:
        """
        ç›´æ¥ä¸Šä¼ æ–‡æœ¬å†…å®¹
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            source_name: æ¥æºåç§°
            
        Returns:
            å¤„ç†ç»“æœæ¶ˆæ¯
        """
        from document_loader import DocumentLoader
        from vector_store import VectorStore
        
        if not text.strip():
            return "âŒ è¯·è¾“å…¥æ–‡æœ¬å†…å®¹"
        
        loader = DocumentLoader(chunk_size=500, chunk_overlap=50)
        docs = loader.load_text_content(text, {"source": source_name})
        
        # åˆ›å»ºæˆ–æ›´æ–°å‘é‡æ•°æ®åº“
        if self.vector_store is None:
            self.vector_store = VectorStore(self.embeddings)
        
        self.vector_store.add_documents(docs)
        self.documents.extend(docs)
        
        # åˆ›å»ºé—®ç­”å¼•æ“
        from qa_engine import QAEngine
        self.qa_engine = QAEngine(
            self.llm,
            self.vector_store.as_retriever(search_kwargs={"k": 3}),
        )
        
        return f"âœ… å·²æ·»åŠ  {len(docs)} ä¸ªæ–‡æ¡£å—\nğŸ“Š çŸ¥è¯†åº“çŠ¶æ€: å…± {len(self.documents)} ä¸ªæ–‡æ¡£å—"
    
    def chat(self, message: str, history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:
        """
        å¯¹è¯
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            history: å¯¹è¯å†å²
            
        Returns:
            (å›å¤, æ›´æ–°åçš„å†å²)
        """
        if not message.strip():
            return "", history
        
        if self.qa_engine is None:
            response = "âš ï¸ è¯·å…ˆä¸Šä¼ æ–‡æ¡£æ„å»ºçŸ¥è¯†åº“"
        else:
            try:
                result = self.qa_engine.ask(message)
                
                # æ ¼å¼åŒ–å›å¤ï¼ˆåŒ…å«æ¥æºï¼‰
                response = result.answer
                if result.sources:
                    sources = set()
                    for doc in result.sources:
                        source = doc.metadata.get("source", "æœªçŸ¥")
                        if isinstance(source, str):
                            sources.add(Path(source).name if "/" in source or "\\" in source else source)
                    if sources:
                        response += f"\n\nğŸ“š å‚è€ƒæ¥æº: {', '.join(sources)}"
            except Exception as e:
                response = f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"
        
        history.append((message, response))
        return "", history
    
    def clear_knowledge_base(self) -> str:
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        self.vector_store = None
        self.qa_engine = None
        self.documents = []
        return "âœ… çŸ¥è¯†åº“å·²æ¸…ç©º"
    
    def get_status(self) -> str:
        """è·å–å½“å‰çŠ¶æ€"""
        if self.vector_store is None:
            return "ğŸ“Š çŸ¥è¯†åº“çŠ¶æ€: æœªåˆå§‹åŒ–\nè¯·ä¸Šä¼ æ–‡æ¡£å¼€å§‹ä½¿ç”¨"
        
        stats = self.vector_store.get_stats()
        return f"""ğŸ“Š çŸ¥è¯†åº“çŠ¶æ€: {stats['status']}
ğŸ“„ æ–‡æ¡£å—æ•°é‡: {stats['document_count']}
ğŸ¤– å¯¹è¯æ¨¡å‹: {IFLOW_MODEL}
ğŸ”¢ Embedding: {SILICONFLOW_EMBEDDING_MODEL}"""


def create_ui():
    """åˆ›å»º Gradio UI"""
    import gradio as gr
    
    app = DocQAApp()
    
    with gr.Blocks(title="æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ") as demo:
        gr.Markdown("""
        # ğŸ“š æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
        
        ä¸Šä¼ æ–‡æ¡£ï¼Œæ„å»ºçŸ¥è¯†åº“ï¼Œè¿›è¡Œæ™ºèƒ½é—®ç­”ã€‚æ”¯æŒ PDFã€Markdownã€TXT æ ¼å¼ã€‚
        """)
        
        with gr.Row():
            # å·¦ä¾§ï¼šæ–‡æ¡£ä¸Šä¼ 
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ æ–‡æ¡£ç®¡ç†")
                
                # æ–‡ä»¶ä¸Šä¼ 
                file_upload = gr.File(
                    label="ä¸Šä¼ æ–‡æ¡£",
                    file_count="multiple",
                    file_types=[".pdf", ".md", ".txt", ".docx"],
                )
                upload_btn = gr.Button("ğŸ“¤ ä¸Šä¼ å¹¶å¤„ç†", variant="primary")
                
                # æ–‡æœ¬è¾“å…¥
                gr.Markdown("---")
                text_input = gr.Textbox(
                    label="æˆ–ç›´æ¥è¾“å…¥æ–‡æœ¬",
                    placeholder="åœ¨è¿™é‡Œç²˜è´´æ–‡æœ¬å†…å®¹...",
                    lines=5,
                )
                text_source = gr.Textbox(
                    label="æ¥æºåç§°",
                    placeholder="ä¾‹å¦‚ï¼šå…¬å¸è§„ç« åˆ¶åº¦",
                    value="ç”¨æˆ·è¾“å…¥",
                )
                text_btn = gr.Button("ğŸ“ æ·»åŠ æ–‡æœ¬")
                
                # çŠ¶æ€æ˜¾ç¤º
                gr.Markdown("---")
                status_display = gr.Textbox(
                    label="ç³»ç»ŸçŠ¶æ€",
                    value=app.get_status(),
                    interactive=False,
                    lines=5,
                )
                
                with gr.Row():
                    refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€")
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºçŸ¥è¯†åº“", variant="stop")
                
                # ä¸Šä¼ ç»“æœ
                upload_result = gr.Textbox(
                    label="å¤„ç†ç»“æœ",
                    interactive=False,
                    lines=3,
                )
            
            # å³ä¾§ï¼šå¯¹è¯
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’¬ æ™ºèƒ½é—®ç­”")
                
                chatbot = gr.Chatbot(
                    label="å¯¹è¯",
                    height=500,
                )
                
                with gr.Row():
                    msg_input = gr.Textbox(
                        label="è¾“å…¥é—®é¢˜",
                        placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                        scale=4,
                        show_label=False,
                    )
                    send_btn = gr.Button("å‘é€", variant="primary", scale=1)
                
                clear_chat_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯")
        
        # ç¤ºä¾‹é—®é¢˜
        gr.Markdown("### ğŸ’¡ ç¤ºä¾‹é—®é¢˜")
        gr.Examples(
            examples=[
                "è¿™ä»½æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                "è¯·æ€»ç»“ä¸€ä¸‹å…³é”®ä¿¡æ¯",
                "æœ‰å“ªäº›é‡è¦çš„æ³¨æ„äº‹é¡¹ï¼Ÿ",
            ],
            inputs=msg_input,
        )
        
        # äº‹ä»¶ç»‘å®š
        upload_btn.click(
            fn=app.upload_files,
            inputs=[file_upload],
            outputs=[upload_result],
        ).then(
            fn=app.get_status,
            outputs=[status_display],
        )
        
        text_btn.click(
            fn=app.upload_text,
            inputs=[text_input, text_source],
            outputs=[upload_result],
        ).then(
            fn=app.get_status,
            outputs=[status_display],
        ).then(
            fn=lambda: "",
            outputs=[text_input],
        )
        
        send_btn.click(
            fn=app.chat,
            inputs=[msg_input, chatbot],
            outputs=[msg_input, chatbot],
        )
        
        msg_input.submit(
            fn=app.chat,
            inputs=[msg_input, chatbot],
            outputs=[msg_input, chatbot],
        )
        
        clear_chat_btn.click(
            fn=lambda: [],
            outputs=[chatbot],
        )
        
        refresh_btn.click(
            fn=app.get_status,
            outputs=[status_display],
        )
        
        clear_btn.click(
            fn=app.clear_knowledge_base,
            outputs=[upload_result],
        ).then(
            fn=app.get_status,
            outputs=[status_display],
        ).then(
            fn=lambda: [],
            outputs=[chatbot],
        )
    
    return demo


if __name__ == "__main__":
    print("=" * 60)
    print("æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    print("=" * 60)
    
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
    )
